{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "        model_name (str): Pretrained model being used\n",
    "        output_classes: Binary classification, so 2\n",
    "        batch_size (int): Batch size\n",
    "        num_epochs (int): Number of epochs\n",
    "        feature_extract (bool): Are we using the CNN as a feature extractor(changing only the final layer)\n",
    "                        or retraining for our problem\n",
    "\n",
    "        Adapted from the pytorch tutorials on using pre-trained models to train new classifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = None\n",
    "    output_classes = 2\n",
    "\n",
    "    def __init__(self, model_name, output_classes = 2, batch_size = 8, num_epochs=15, feature_extract=True):\n",
    "        self.model_name = model_name\n",
    "        self.output_classes = output_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.feature_extract = feature_extract\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def train_model(self, model, criterion, optimizer, dataloaders, dataset_sizes, is_inception=False):\n",
    "        since = time.time()\n",
    "\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        best_accuracy = 0.0\n",
    "        val_acc_history = []\n",
    "\n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, self.num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "   \n",
    "                        if is_inception and phase == 'train':\n",
    "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                            outputs, aux_outputs = model(inputs)\n",
    "                            loss1 = criterion(outputs, labels)\n",
    "                            loss2 = criterion(aux_outputs, labels)\n",
    "                            loss = loss1 + 0.4*loss2\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    # if phase == 'train':\n",
    "                    #     scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                if phase == 'train':\n",
    "                    per_epoch_accuracy.append(epoch_acc)\n",
    "                    per_epoch_loss.append(epoch_loss)\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_accuracy:\n",
    "                    best_accuracy = epoch_acc\n",
    "                    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_accuracy))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        return model, val_acc_history, per_epoch_loss, per_epoch_accuracy\n",
    "\n",
    "\n",
    "    def set_requires_grad(self, model):\n",
    "        if self.feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def initPretrainedModel(self, inputSize):\n",
    "        model = None\n",
    "        input_size = 0\n",
    "        if self.model_name == 'alexnet' and self.feature_extract:\n",
    "            model = torchvision.models.alexnet(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.classifier[6].in_features\n",
    "            model.classifier[6] = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        if self.model_name == 'inception' and self.feature_extract:\n",
    "            print(\"Initializing model: Inception_V3\")\n",
    "            model = models.inception_v3(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            # Handle the auxilary net\n",
    "            num_ftrs = model.AuxLogits.fc.in_features\n",
    "            model.AuxLogits.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            # Handle the primary net\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "        \n",
    "        if self.model_name == \"vgg\" and self.feature_extract:\n",
    "            model = models.vgg11_bn(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.classifier[6].in_features\n",
    "            model.classifier[6] = nn.Linear(num_ftrs,self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        if self.model_name == 'densenet' and self.feature_extract:\n",
    "            \"\"\"\n",
    "            Densenet 121\n",
    "            \"\"\"\n",
    "            print(\"Initializing to use pre-trained DenseNet 121 for feature extraction...\")\n",
    "            model = models.densenet121(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.classifier.in_features\n",
    "            model.classifier = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "            \n",
    "        if self.model_name == \"resnet\":\n",
    "            \"\"\"Resnet 18\n",
    "            \"\"\"\n",
    "            print(\"Initializing to use pre-trained Resnet 18 for feature extraction...\")\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        return model\n",
    "\n",
    "    def testModel(self, dataloaders, model, classes, dataset_sizes, batch_size):\n",
    "        correct = 0\n",
    "        total = dataset_sizes['test']\n",
    "        predictions = []\n",
    "\n",
    "        y_actual = []\n",
    "        y_pred = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, (inputs, labels) in enumerate(dataloaders['test'], 0):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                                \n",
    "                samples = dataloaders['test'].dataset.samples[index*batch_size : index*batch_size + batch_size]\n",
    "                predicted_classes = [classes[predicted[j]] for j in range(predicted.size()[0])]\n",
    "                sample_names = [s[0] for s in samples]\n",
    "                \n",
    "                predictions.extend(list(zip(sample_names, predicted_classes)))\n",
    "                # labels = labels.cpu()\n",
    "                # predicted = predicted.cpu()\n",
    "                y_actual.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        try:\n",
    "            print(f\"Accuracy (Sklearn): {sklearn.metrics.accuracy_score(y_actual, y_pred)}\")\n",
    "            print(f\"F1-Score (Sklearn): {sklearn.metrics.f1_score(y_actual, y_pred)}\")\n",
    "            print(f\"Precision Score: {sklearn.metrics.precision_score(y_actual, y_pred)}\")\n",
    "            print(f\"Recall Score: {sklearn.metrics.recall_score(y_actual, y_pred)}\")\n",
    "            print(f\"\\nConfusion Matrix:\\n{sklearn.metrics.confusion_matrix(y_actual, y_pred)}\")\n",
    "            print(f\"\\nClassification Report:\\n{sklearn.metrics.classification_report(y_actual, y_pred)}\")\n",
    "        except RuntimeError:\n",
    "            print(\"Error computing metrics: \\n\", RuntimeError)\n",
    "\n",
    "        print('\\n\\nAccuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class DataSet:\n",
    "    \"\"\"\n",
    "    Note: Ensure data directory is in ImageFolder format.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    data_dir = None\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def initDataLoaders(data_dir, batch_size):\n",
    "        data_transforms = DataSet.setUpDataLoaderTransformers()\n",
    "        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                                    shuffle=True, num_workers=4)\n",
    "                    for x in ['train', 'val']}\n",
    "        dataloaders['test'] = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size,\n",
    "                                                    shuffle=False, num_workers=4)\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "        class_names = image_datasets['train'].classes\n",
    "\n",
    "        return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "    @staticmethod\n",
    "    def setUpDataLoaderTransformers(inputSize = 224):\n",
    "                \n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(inputSize),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(inputSize),\n",
    "                transforms.CenterCrop(inputSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(inputSize),\n",
    "                transforms.CenterCrop(inputSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "    \n",
    "        # data_transforms = {\n",
    "        #     'train': transforms.Compose([\n",
    "        #         transforms.Resize((inputSize,inputSize)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        #     'val': transforms.Compose([\n",
    "        #         transforms.Resize((inputSize,inputSize)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        #     'test': transforms.Compose([\n",
    "        #         transforms.Resize((224,224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        # }\n",
    "\n",
    "        return data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from classifier import Classifier\n",
    "from optimizer import Optimizer\n",
    "from dataset import DataSet\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_dir = 'data'\n",
    "    model_name = 'densenet'\n",
    "    output_classes = 2\n",
    "    feature_extract = True\n",
    "    batch_size = 8\n",
    "    num_epochs = 20\n",
    "\n",
    "    learningRate = 0.001\n",
    "    momentum = 0.9\n",
    "\n",
    "    run_id = 'l_' + str(learningRate) + '_m_' + str(momentum)\n",
    "\n",
    "#     saved_data_structures_dir = 'saved_data_structures/'\n",
    "\n",
    "    densenetClassifier = Classifier(model_name, output_classes, batch_size, num_epochs, feature_extract)\n",
    "    model = densenetClassifier.initPretrainedModel(224)\n",
    "\n",
    "    dataloaders_dict, dataset_sizes, class_names = DataSet.initDataLoaders(data_dir, batch_size)\n",
    "    data_transforms = DataSet.setUpDataLoaderTransformers()\n",
    "\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in\n",
    "                      ['train', 'val', 'test']}\n",
    "\n",
    "    # Detect if we have a GPU available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sgdOptimizer = Optimizer(device)\n",
    "    optimizer_ft = sgdOptimizer.optimize(model, feature_extract, learningRate, momentum)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model, val_acc_history, per_epoch_loss, per_epoch_accuracy = densenetClassifier.train_model(model,\n",
    "                                                criterion,\n",
    "                                                optimizer_ft,\n",
    "                                                dataloaders_dict,\n",
    "                                                dataset_sizes)\n",
    "\n",
    "    print(f\"Validation Accuracy History:\\n{val_acc_history}\")\n",
    "    print(f\"\\nPer epoch loss:\\n{per_epoch_loss}\")\n",
    "    print(f\"\\nPer epoch accuracy:\\n{per_epoch_accuracy}\")\n",
    "\n",
    "    # Save lists for plots:\n",
    "    print(\"Saving per_epoch_losses, per_epoch_accuracy to disk for analysis...\")\n",
    "    epoch_losses_file = 'epoch_losses_' + run_id + '.pickle'\n",
    "    with open(epoch_losses_file, 'wb') as handle:\n",
    "        pickle.dump(per_epoch_loss, handle)\n",
    "\n",
    "    epoch_accuracies_file = 'epoch_accuracies_' + run_id + '.pickle'\n",
    "    with open(epoch_accuracies_file, 'wb') as handle:\n",
    "        pickle.dump(per_epoch_accuracy, handle)\n",
    "\n",
    "    val_acc_history_file = 'val_acc_history_' + run_id + '.pickle'\n",
    "    with open(val_acc_history_file, 'wb') as handle:\n",
    "        pickle.dump(val_acc_history, handle)\n",
    "\n",
    "    print(\"Saving final model to disk...\")\n",
    "    save_as_name = 'densenetFeatureExtraction_' + run_id + '.pt'\n",
    "    torch.save({\n",
    "        'name': 'densenet_feature_extraction_' + run_id,\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "    }, save_as_name)\n",
    "\n",
    "    # testing\n",
    "    print(\"Running independent test...\")\n",
    "    state = torch.load(save_as_name)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    predictions = densenetClassifier.testModel(dataloaders_dict, model, class_names, dataset_sizes, batch_size=8)\n",
    "\n",
    "    # save predicted values\n",
    "    save_as_name = 'predictedLabelsDensenet_' + run_id + '.csv'\n",
    "    np.savetxt(save_as_name, predictions, fmt='%s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
